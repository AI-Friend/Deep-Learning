# <center>NLG之生成相似句子</center>

<center>Li Junli</center>

<center>Apr 2019</center>

​	

​	NLG（自然语言生成）是自然语言处理两大领域之一, 是研究计算机程序如何根据信息生成一段高质量的自然语言文本。 在计算机语言学领域内, 文本自动生成的过程分为两个阶段:内容决定阶段和表示层生成阶段。NLG主要有两个目的:第一, 作为人们生活中的交流、交际工具, 利用日常语言知识和领域知识来生成文本、分析报告、帮助消息等。第二, 作为检验给定的特定自然语言理论的一种技术手段。

​	本次的NLU意图识别目标，因训练样本量不足需要用类似自然语言生成的技术扩大样本量。另外，意图识别训练完成后，需要在NLU的基础上根据学习到的策略来生成对话回复。



## 自然语言生成技术

​	(1)模板生成技术(Template-based Generation)

​	模板生成技术是人们最早采用的一种自然语言生成技术, 也是一种最简单的技术, 这种生成技术的原理和填充方法相似, 系统事先设计好几种可能出现的语言情况, 构造几个相应的模板, 每个模板包括一些常量和一些变量, 当用户输入一定的信息后, 文本生成器将这些信息作为字符串嵌入到模板中替代变量。这种生成器称为非语言的文本生成器, 因为它的处理只是在字符串的水平上, 没有在深层次上进行语言处理, 所以文本有一定的不完备性。这种技术虽然思路简单, 但生成的文本质量不高。不过它目前仍具有十分广泛的用途。例如:有许多应用软件都采用该技术处理出错信息、警示信息。该方法的优点是作效率高、实现手段简单。缺点是生成的文质量不高, 难以满足人们多变的需要,不能体问题具体分析地生成文本。其次, 使用模技术的系统维护、修改或扩充都十分困难。

​	(2)模式生成技术(Schema-based Generation)

​	Schema技术是基于语言学中的修辞谓词(Predicate)来表达文本结构的一种方法, 这种方法将文本中的句子功能进行分类, 并把标准模式和修辞性谓语相结合, 使它们具有完备性。同时, 这种方法采用Predicate来描述文本结构的规律, 是文本的基本结构框架的表示, 在该表示中也明确了会话中心内容的表达顺序。文本都是由主题组成的, 主题是指一个句子或者一个从句Predicate根据文本中的主题进行分类, 每个主题都被归纳为特定的Predicate。对一些特定的文本而来说, 存在着一些标准的Predicate组合模式来表示文本的基本结构, 这种模式就称为Schema。其结构树中节点一般分为五种类型:Root, Schema, Predicate, Argument和Modifier。其中, Root是根节点, 表示一篇文章。每个Root下面有若干个Schema子节点, 一个Schema表示一个段落或几句话, Schema下面的子节点可以继续是Schema, 也可以是Predicate。而一棵以Predicate为根的树表示一个句子, 它是文章的基本单位, 句子中每一个基本语义成分均是Predicate的子节点, 用Argument表示。若Argument有修饰成分, 则用子节点Modifier标志。Argument或Modifier是树的叶子节点, 树中每个节点都含有若干个槽, 用来标志各种信息以供生成使用。模式生成技术具有较好的维护性, 输出的文本质量较高。它的缺点是只用于固定结构段落, 生成的文本不灵活。

​	(3)短语/规则扩展技术(Phrase/Plan Expansion)

​	RST(Rhetorical Structure Theory)生成技术是基于Mann和Thompson提出的关于描述文本结构的修辞结构理论, RST理论认为一篇文章的各个组成部分无论是句子、段落甚至更大的组成单位之间都是由一些特定的关系按照一定的层次内聚在一起的。多数NLG生成系统都包含一个修辞关系集, 而与具体应用相对应的关系集是其子集。到目前为止, RST包含的基本关系有Nucleus-Satellite和Multi-Nucleus两种模式。其中Nucleus-Satellite模式包括核心部分(Nucleus)和附属部分(Satellite), 核心部分表达基本命题, 附属部分表达一个附属命题,一般用于描述目的、因果、转折、递进、背景等关系;Multi-Nucleus模式涉及一个或多个语段, 它没有附属部分, 多用于描述顺序、并列等关系。Phrase/Plan技术比Schema技术具有更强的灵活性, 在子树的生成过程中, 同时也就生成了文本的总体框架结构。主要缺点是它的基本数据结构、文本规则库的建立有一定难度, 因为句子之间有很多语法和语义上面的联系, 所以对句与句之间内部关系必须仔细考虑限制, 应当防止不恰当的扩展。

​	(4)属性特征生成技术

​	从某种意义上讲, 属性特征生成技术是比较难实现的一项技术。在这些系统中, 每一个细小的变化部分都由一个简单的属性特性表示出来。例如:输出的一句话的语气是主动还是被动, 它的语气动作是问题还是命令或者是一个声明。这都将被属性特征表示出来, 甚至输出的每个单元都与一个特定的唯一的属性特征集相连, 达到一一对应, 这需要大量细致的工作实现。而且输出过程是对要生产的每个信息部分增加相应的属性特征, 一直到能够唯一地决定一个输出结果为止。然后在经过一个线性的处理过程, 将一串属性特征集变成线性的符号串。在句子一级水平上, 特征属性是语法特征, 输出的符号是词汇。该方法的优点是概念简单, 任何一种不同类的语言都能轻易地作为特征加入进去。生成的文本相当灵活。其缺点是很难维护各属性之间的内容关系, 难以控制特征集的选择, 并且工作量很​	

​	(5)基于学习的自然语言生成技术——视觉接地技术　　
​	前面的自然语言生成系统无论在语法词汇还是信息策划处理方面都不具备学习能力，视觉接地技术可弥补这一缺陷，视觉接地指的是将语言和用户的语境中相关的事物联系起来处理的过程。这种系统是采用一种“展示并告诉”的程序来训练的, 该程序的过程是视觉场景搭配自然语言描述。该学习算法将得到一种概率结构, 这种结构对词语结构的视觉语义、词类以及个别的词语进行编码。利用这种概率结构, 一种包含语法、语义和语境限制的算法可以形成自然并且没有歧义的物体描述。

​	自然语言生成系统形成了形容词和名词构成的名词短语, 同时也形成了空间从句。需要的语言学结构将通过训练数据来产生,还可以处理训练过程中没有出现过的新的词语序列。这个语言生成系统的输出是通过综合扫描基于词语的原始训练语料库来实现的。从人类的判断来评价, 这种自动生成的语言描述的性能可以和人类的语言描述性能相媲美。

​	视觉接地技术实现过程:

​	①数据准备, 由于在自然语言产生之前, 在计算机会呈现各式各样不同的目标物体, 所以要尽可能多地记录下有关目标物体的一切行为。

​	②词语聚类, 词语聚类包括3 种基本的聚类方式:分布聚类, 当两种词语出现在同一语句中, 这两种词语相斥, 不是同一类词语, 反之成立。基于语义特征连接的聚类, 这种聚类方法忽略了词语是否同时在一语句中出现, 主要研究词语和其特征的语义联系, 这种聚类方法的主要目的是对那些与视觉特征有接地联系的词语进行聚类。混合聚类, 在词语聚类时要同时考虑词语是否共同出现和词语与视觉特征的语义联系。

​	③特征选择, 将特征分配给每一个独立的词语, 把一类词语获得的特征作为其他类词语选择特征的连接。

​	④建立语义模型, 将词语获得的特征量化, 建立不同特征的高斯模型, 并在不同的高斯区域标明相应的目标物体的状态。当一个新目标物体出现时, 首先提取它的特征值, 然后根据特征值所在的高斯区域就可以判断出新目标物体的状态。

​	⑤产生物体描述语言, 这是视觉接地技术的最后一个阶段, 希望可以产生形成接地语言模型的物体描述语言。要产生这种语言就要转化为一种约束条件搜索问题。这里有3种约束条件需要整合搜索, 分别是语法约束条件、语义约束条件和语境约束条件。语法约束条件主要用于产生和句法相容的词语;语义约束条件可以描述目标物体的特征;语境约束条件搜索可以降低目标物体相对于其他物体的模糊度。只要将这3种约束条件整合起来进行搜素, 就可以实现物体的语言描述。

​	视觉接地技术的优点是不仅具有学习记忆功能, 而且还在语言产生的过程, 对要表达的信息进行了语义和句法方面的聚合, 这是当今研究的重点。视觉接地技术的缺点就是在数据准备阶段要人工记录目标物体的行为状态, 这将浪费一定的人力和物力。　　



## 生成相似句子

​	结合自然语言生成技术和需求，用Word Embedding 寻找相似词，进而生成相似句子是比较合适的方法。

​	详细步骤如下：

​	一、把输入的短语或者句子分词；

​	二、对每个中文单词，在类似Chinese Word Vectors这样的已经训练好的词向量集中找到和它语义接近的其它汉语单词。对于任意两个已经用Word Embedding形式表示的单词，可以简单通过计算两个向量之间的Cosine相似性，得出两个单词语义接近程度。比如通过计算可以得出如下单词对的语义相似性：

     Cosine(WE“林志玲”，WE“舒淇”) =0.93
    
     Cosine(WE“教练”，WE“领队”) =0.81
    
     Cosine(WE“星期二”，WE“星期四”) =0.93
​	对于输入句子的某个单词，可以从所有其它单词中找出和这个单词语义最接近的一部分单词，也就是Cosine得分最高的一批单词。

​	三、对某个单词W，找出语义最接近的单词列表后对其进行过滤，过滤规则是：根据词性过滤，把这些单词中词性和W相同的留下来，不同的过滤掉。这步是很关键的，对于后面最终产生的句子语义一致性及可读性有很大影响。主要原因是，尽管理论上通过Word Embedding可以找到语义相似的其它单词，但是其实还是有不少看上去不合理的内容，这是Word Embedding本身产生方式决定的，无法避免，增加合理的过滤措施能够极大改善句子生成质量，而根据词性过滤就是一个简单易行效果也不错的小特技。

​	四、通过上述办法，输入句子中的每个单词都找出对应的语义接近且词性相同的单词列表。

​	五、假设输入句子包含三个单词“W1、W2、W3”，W1找出了10个语义接近的单词，W2找出了8个，W3找出了10个，那么对这些单词按W1、W2、 W3原始顺序进行排列组合，那么会有10 * 8 * 10 = 800 个可能的组合。这些组合就是根据“W1、W2、W3”生成的各种语义可能句子的所有句子语义空间。


​	六、需要一种度量标准来对这些句子进行打分，给出看上去最合理的句子作为输出结果，可以用语言模型来实现。语言模型本质上是衡量一个句子表达流畅性的技术工具，就是说给了一句话，语言模型可以判断这句话是像人话还是像鬼话，如果越像人话，那么语言模型给这个句子的打分越高。语言模型是NLP中非常广泛使用的工具，比如机器翻译、语音识别、输入法等等都是必需要用语言模型来评估句子流畅性的。例如通过语言模型给下面两个句子打分，就可以判断出哪个句子读起来更合理：


​	 LanguageModel(“我最喜欢看知音杂志”)  > LanguageModel(“我喜欢最知音看杂志”)

​	可通过LSTM模型训练出语言模型，然后用来判断各种排列组合出的句子哪些更合理，选择得分最高的一部分输出作为最终的句子产生结果。从上面例子可以看出，如果输入句子比较长的话，产生的单词组合空间是非常非常巨大的，对每个句子依次进行语言模型运算速度会非常慢，此时可以引入类似Beam Search的思路来大幅提升计算速度。

​	经过上述六个步骤，给定一个输入句子，可以生成这个句子语义接近的中文句子了。



## 参考文献与链接

[1] Deb Roy. Grounded speech communication. Proceedings of the International Conference on Spoken 

​      Language processing, 2000.

[2] 王纤.自然语言生成系统的实现技术分析[J] .研究与设计,1997, 4:51 -54.

[3] 张建华, 陈家俊.自然语言生成综述[J] .计算机应用研究, 2006, 8:1 -3.

[4] 史忠植.智能科学[M] .北京:清华大学出版社, 2006.

\[5] [机器之心——自然语言生成](https://www.jiqizhixin.com/graph/technologies/f04b46c6-6e93-4912-822d-bcecd5528d65)

\[6] [LSTM与Softmax进行意图识别]([使用LSTM和Softmx来进行意图识别](https://www.cnblogs.com/ModifyRong/p/8546421.html))

\[7] [padlepadle进行意图识别](https://www.cnblogs.com/ModifyRong/p/8231899.html)

\[8] [Word Embedding 自动生成相似句子](https://blog.csdn.net/malefactor/article/details/50767711)









